{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mz format corpus\n",
    "datasets = [\"train_2011\", \"test_2011\", \"train_2013\", \"test_2013\"]\n",
    "years = [\"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "w2i = {}\n",
    "i = 0\n",
    "for dataset, year in zip(datasets, years):\n",
    "    fn = \"/u4/w85yang/deep-tweet-search/data/twitter/order_by_rel/{}\".format(dataset)\n",
    "    # In fact, a.toks does not increase the vocabulary size\n",
    "    a = open(os.path.join(fn, \"a.toks\"))\n",
    "    b = open(os.path.join(fn, \"b.toks\"))\n",
    "    c = open(os.path.join(fn, \"id.txt\"))\n",
    "    sim = open(os.path.join(fn, \"sim.txt\"))\n",
    "    with open(\"TweetCorpus/{}.txt\".format(year), \"w\") as out:\n",
    "        for l1, l2, l3, l4 in zip(a, b, c, sim):\n",
    "            qid, iternum, docno, aid, undefined, run_id = l3[:-1].split()\n",
    "            out.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(l4[:-1], l1[:-1], l2[:-1], qid, docno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_val(years, test_year, num_samples, val_split=0.1):\n",
    "    \"\"\"\n",
    "    :param datasets: list of training set names to be sampled\n",
    "    :param num_samples: total number of training samples\n",
    "    :param val_split: ratio of validation set\n",
    "    :return: indices of validation samples\n",
    "    \"\"\"\n",
    "    a = open(\"TweetCorpus/train_{}.txt\".format(test_year), \"w\")\n",
    "    b = open(\"TweetCorpus/dev_{}.txt\".format(test_year), \"w\")\n",
    "    count = num_samples * val_split / 3\n",
    "    sample_docids = []\n",
    "    for year in years:\n",
    "        c = open(\"/u4/w85yang/RankLib/trunk/data/{}.txt\".format(year))\n",
    "        last_qid = \"1\"\n",
    "        qid2docid = []\n",
    "        for ind, l in enumerate(c):\n",
    "            qid = l[:0-1].split()[1].split(':')[1]\n",
    "            if qid != last_qid:\n",
    "                qid2docid.append(ind)\n",
    "                last_qid = qid\n",
    "        qid2docid.append(ind + 1)\n",
    "        \n",
    "        sampled_qids = set()\n",
    "        num = 0\n",
    "        while (True):\n",
    "            sample_qids = random.sample(list(range(len(qid2docid))), 1)\n",
    "            # print(\"sampled qid: {} in dataset {}\".format(sample_qids, data_name))\n",
    "            qid = sample_qids[0]\n",
    "            if qid in sampled_qids:\n",
    "                continue\n",
    "            sampled_qids.add(qid)\n",
    "            startInd = qid2docid[qid - 1] if qid != 0 else 0\n",
    "            endInd = qid2docid[qid]\n",
    "            num += endInd - startInd\n",
    "            if num > count * 1.05:\n",
    "                print(\"validate {} samples in dataset {}\".format(num, year))\n",
    "                break\n",
    "            sample_docids.extend(range(startInd, endInd))\n",
    "            if num > count * 0.95:\n",
    "                print(\"validate {} samples in dataset {}\".format(num, year))\n",
    "                break\n",
    "        \n",
    "        c = open(\"TweetCorpus/{}.txt\".format(year))\n",
    "        for ind, l in enumerate(c):\n",
    "            if ind in sample_docids:\n",
    "                b.write(l)\n",
    "            else:\n",
    "                a.write(l)\n",
    "    print(\"validate {} samples in total\".format(len(sample_docids)))\n",
    "    return sample_docids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate 4622 samples in dataset 2012\n",
      "validate 4801 samples in dataset 2013\n",
      "validate 5128 samples in dataset 2014\n",
      "validate 13739 samples in total\n",
      "validate 4201 samples in dataset 2011\n",
      "validate 4651 samples in dataset 2013\n",
      "validate 4670 samples in dataset 2014\n",
      "validate 11974 samples in total\n",
      "validate 4201 samples in dataset 2011\n",
      "validate 4879 samples in dataset 2012\n",
      "validate 4670 samples in dataset 2014\n",
      "validate 12186 samples in total\n",
      "validate 5111 samples in dataset 2011\n",
      "validate 4895 samples in dataset 2012\n",
      "validate 4922 samples in dataset 2013\n",
      "validate 12204 samples in total\n"
     ]
    }
   ],
   "source": [
    "# generate train-dev sets for each year\n",
    "years2 = [\"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "for test_year in years2:\n",
    "    random.seed(123456789)\n",
    "    np.random.seed(123456789)\n",
    "    years = [\"2011\", \"2012\", \"2013\", \"2014\"]\n",
    "    years.remove(test_year)\n",
    "    dataset2count = {\"2011\": 39780, \"2013\": 46192, \"2012\": 49879, \"2014\": 41579}\n",
    "    num_samples = 0\n",
    "    for year in years:\n",
    "        num_samples += dataset2count[year]\n",
    "\n",
    "    sample_docids = sample_val(years, test_year, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
